{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: 그랬더니 해방 후에 또 ○○○○들이 전처럼 드나듭니다.\n",
      "생성 1: \n",
      "생성 2: \n",
      "생성 3: \n",
      "생성 4: \n",
      "생성 5: \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline ,BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"./Similar_bart_prototype_model\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "BOS=tokenizer.bos_token\n",
    "EOS =tokenizer.eos_token\n",
    "PAD = tokenizer.pad_token\n",
    "\n",
    "replacements = {BOS: \"\", EOS: \"\", PAD: \"\"}\n",
    "\n",
    "def generate_sequence(context, model, tokenizer, device, num_sequences=2):\n",
    "      # 모델을 평가 모드로 설정\n",
    "\n",
    "    # 입력 문맥 토크나이징\n",
    "    input_ids = tokenizer.encode(BOS + context + EOS, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 모델을 사용하여 출력 생성\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=1024,\n",
    "        temperature=0.7,\n",
    "        top_k=5,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=2.0,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_sequences\n",
    "    )\n",
    "\n",
    "    generated_sequences = []\n",
    "    for generated_sequence in output_sequences:\n",
    "        # 생성된 시퀀스를 디코딩하여 텍스트로 변환\n",
    "        decoded_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        generated_sequences.append(decoded_text)\n",
    "    \n",
    "    return generated_sequences\n",
    "\n",
    "def multiple_replace(text, replacements):\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "model.eval()\n",
    "context = \"\" # 여기에 input값을 넣으세요\n",
    "\n",
    "# context를 input으로 넣으면 output을 3개 생성함\n",
    "generated_sentences = generate_sequence(context, model, tokenizer, device, num_sequences=3)\n",
    "\n",
    "for idx, sentence in enumerate(generated_sentences):\n",
    "    sentence = multiple_replace(sentence, replacements)\n",
    "    print(f\"생성 {idx+1}:\", sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
