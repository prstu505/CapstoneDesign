{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer ,PreTrainedTokenizerFast\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['para'])\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Similar_first_input_label_data.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[\"data\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "거미줄\n"
     ]
    }
   ],
   "source": [
    "print(data[\"data\"][0][\"para\"][0][\"label\"][0][\"token_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64645\n"
     ]
    }
   ],
   "source": [
    "Similar_raw = []\n",
    "\n",
    "for item in data[\"data\"]:\n",
    "    for para in item[\"para\"]:\n",
    "        context = para[\"context\"]\n",
    "        for labels in para[\"label\"]:\n",
    "            label = labels[\"sequence\"]\n",
    "            token_str = labels[\"token_str\"]\n",
    "            Similar_raw.append((context,label,token_str))\n",
    "\n",
    "print(len(Similar_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58180 6465\n"
     ]
    }
   ],
   "source": [
    "train_data_raw , test_data_raw = train_test_split(Similar_raw,test_size = 0.1)\n",
    "\n",
    "print(len(train_data_raw),len(test_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미친 사람같이 대귈뜰에 매 부르는 소리를 하면서 돌아다녔다.</td>\n",
       "      <td>미친 거 같이 매 부르는 소리를 하면서 돌아다녔다.</td>\n",
       "      <td>거</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>만두 같은 달이건만 보는 이의 심경에 따라 기쁘기도 하고 슬프기도 한격이라고 할는지요.</td>\n",
       "      <td>아마 같은 달이건만 보는 이의 심경에 따라 기쁘기도 하고 슬프기도 한격이라고 할는지요.</td>\n",
       "      <td>아마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>가을 산골 벼가 금같이 누러니 저녁놀이 자주 같이 붉으니 아버지 살결이 유달리 희다</td>\n",
       "      <td>가을 산골 벼가 늘 같이 누러니 저녁놀이 자주 같이 붉으니 아버지 살결이 유달리 희다</td>\n",
       "      <td>늘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>너는 돼지처럼 기름지냐.</td>\n",
       "      <td>너는 너 처럼 기름지냐.</td>\n",
       "      <td>너</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그의 몸은 사시나무와 같이 떨렸다.</td>\n",
       "      <td>그의 몸은 공기 와 같이 떨렸다.</td>\n",
       "      <td>공기</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            context  \\\n",
       "0                 미친 사람같이 대귈뜰에 매 부르는 소리를 하면서 돌아다녔다.   \n",
       "1  만두 같은 달이건만 보는 이의 심경에 따라 기쁘기도 하고 슬프기도 한격이라고 할는지요.   \n",
       "2    가을 산골 벼가 금같이 누러니 저녁놀이 자주 같이 붉으니 아버지 살결이 유달리 희다   \n",
       "3                                     너는 돼지처럼 기름지냐.   \n",
       "4                               그의 몸은 사시나무와 같이 떨렸다.   \n",
       "\n",
       "                                              label token  \n",
       "0                      미친 거 같이 매 부르는 소리를 하면서 돌아다녔다.     거  \n",
       "1  아마 같은 달이건만 보는 이의 심경에 따라 기쁘기도 하고 슬프기도 한격이라고 할는지요.    아마  \n",
       "2   가을 산골 벼가 늘 같이 누러니 저녁놀이 자주 같이 붉으니 아버지 살결이 유달리 희다     늘  \n",
       "3                                     너는 너 처럼 기름지냐.     너  \n",
       "4                                그의 몸은 공기 와 같이 떨렸다.    공기  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame(train_data_raw)\n",
    "test_data = pd.DataFrame(test_data_raw)\n",
    "print(len(train_data))\n",
    "train_data.rename(columns={0: 'context',1:'label',2:'token'}, inplace=True)\n",
    "test_data.rename(columns={0: 'context',1:'label',2:'token'}, inplace=True)\n",
    "\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그는 은실 금실로 수놓은 끝동 소매를 조금 치켜서 옥 같은 손으로 뱃전을 짚고 그 ...</td>\n",
       "      <td>그는 은실 금실로 수놓은 끝동 소매를 조금 치켜서 고사리 같은 손으로 뱃전을 짚고 ...</td>\n",
       "      <td>고사리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그의 명성이 떨치고, 문하 구름같이 모여들었다.</td>\n",
       "      <td>그의 명성이 떨치고, 문하 하나 같이 모여들었다.</td>\n",
       "      <td>하나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>창수는 혼자 이렇게 중얼대며 신작로 넓은 길에 활 등처럼 굽은 S 산 밑 송림 사이...</td>\n",
       "      <td>창수는 혼자 이렇게 중얼대며 신작로 넓은 길에 활 다리 처럼 굽은 S 산 밑 송림 ...</td>\n",
       "      <td>다리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이슬방울이 우수수 떨어지며, 흙 새에 끼었던 흰 모래알이 의붓자식처럼 한 귀퉁이에 ...</td>\n",
       "      <td>이슬방울이 우수수 떨어지며, 흙 새에 끼었던 흰 모래알이 의붓 조각 처럼 한 귀퉁이...</td>\n",
       "      <td>조각</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>탄환처럼도 의장하려라!</td>\n",
       "      <td>이 처럼도 의장하려라!</td>\n",
       "      <td>이</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  그는 은실 금실로 수놓은 끝동 소매를 조금 치켜서 옥 같은 손으로 뱃전을 짚고 그 ...   \n",
       "1                         그의 명성이 떨치고, 문하 구름같이 모여들었다.   \n",
       "2  창수는 혼자 이렇게 중얼대며 신작로 넓은 길에 활 등처럼 굽은 S 산 밑 송림 사이...   \n",
       "3  이슬방울이 우수수 떨어지며, 흙 새에 끼었던 흰 모래알이 의붓자식처럼 한 귀퉁이에 ...   \n",
       "4                                       탄환처럼도 의장하려라!   \n",
       "\n",
       "                                               label token  \n",
       "0  그는 은실 금실로 수놓은 끝동 소매를 조금 치켜서 고사리 같은 손으로 뱃전을 짚고 ...   고사리  \n",
       "1                        그의 명성이 떨치고, 문하 하나 같이 모여들었다.    하나  \n",
       "2  창수는 혼자 이렇게 중얼대며 신작로 넓은 길에 활 다리 처럼 굽은 S 산 밑 송림 ...    다리  \n",
       "3  이슬방울이 우수수 떨어지며, 흙 새에 끼었던 흰 모래알이 의붓 조각 처럼 한 귀퉁이...    조각  \n",
       "4                                       이 처럼도 의장하려라!     이  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'}\n",
      "['▁그는', '▁은', '실', '▁금', '실로', '▁수', '놓은', '▁끝', '동', '▁소', '매를', '▁조금', '▁치', '켜', '서', '▁옥', '▁같은', '▁손으로', '▁뱃', '전을', '▁짚', '고', '▁그', '▁날', '씬', '한', '▁허', '리를', '▁반', '나마', '▁배', '▁밖으로', '▁기울', '였다.', '▁\"']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2',)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "print(tokenizer.tokenize(test_data[\"context\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = tokenizer.bos_token\n",
    "EOS = tokenizer.eos_token\n",
    "PAD = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import re\n",
    "#데이터 처리 클래스\n",
    "class Similar_Dataset(Dataset):\n",
    "    def __init__(self, chats, max_len=1024):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.bos= BOS\n",
    "        self.eos = EOS\n",
    "        self.pad = PAD\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # 로드한데이터를 차례차례 DataLoader로 넘김\n",
    "        turn = self._data.iloc[idx]\n",
    "        l = turn[\"label\"]  # label을 가져온다.\n",
    "        c = turn[\"context\"]  # 문맥을 가져온다.\n",
    "        \n",
    "        c_toked = self.tokenizer.tokenize(self.bos + c + self.eos)\n",
    "        c_len = len(c_toked)\n",
    "        \n",
    "        l_toked = self.tokenizer.tokenize(self.bos + l + self.eos)\n",
    "        l_len = len(l_toked)\n",
    "\n",
    "        #최대길이 초과시 처리\n",
    "        if c_len> self.max_len:\n",
    "            c_len = self.max_len\n",
    "            c_toked = c_toked[:c_len]\n",
    "       \n",
    "        if l_len> self.max_len:\n",
    "            l_len = self.max_len\n",
    "            l_toked = l_toked[:l_len]\n",
    "\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(l_toked)        \n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        decoder_ids = self.tokenizer.convert_tokens_to_ids(l_toked)    \n",
    "        \n",
    "        while len(decoder_ids) < self.max_len:\n",
    "            decoder_ids += [self.tokenizer.pad_token_id]\n",
    "        \n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(c_toked)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        return (token_ids, decoder_ids, labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    decoder_input = [item[1] for item in batch]\n",
    "    label = [item[2] for item in batch]\n",
    "    return torch.LongTensor(data), torch.LongTensor(decoder_input), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Similar_Dataset(train_data, max_len=1024)\n",
    "loader = DataLoader(train_set, batch_size=4,  num_workers=0, shuffle=True, collate_fn=collate_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14545"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Token IDs: tensor([[    1, 18769, 14949,  ...,     3,     3,     3],\n",
      "        [    1, 14306, 10897,  ...,     3,     3,     3],\n",
      "        [    1, 22505, 10830,  ...,     3,     3,     3],\n",
      "        [    1, 16938, 14836,  ...,     3,     3,     3]])\n",
      "1024\n",
      "torch.Size([4, 1024])\n",
      "tensor(19089)\n",
      "가고\n",
      "</s> 그렇다면 왕은 수렵이라도 가고 궁전만은 비어 있는 것일까 하고 돌축을 하나하나 밟아 가면 또다시 기다란 줄 행랑이 축을 하나하나 밟아 가면 또다시 기다란 줄 행랑이 있는 것이고, 그것을 오른편으로 돌아들어 왼편으로 보이는 별실은 서재인 듯 조용한 목에 뜰 앞에 조롱들 속에서 빛깔 다른 새들이 시스마금 낯선 손님을 맞아 아는 체하고 재재거린다.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "4\n",
      "Decoder_ids: tensor([[    1, 18769, 14949,  ...,     3,     3,     3],\n",
      "        [    1, 14306, 10897,  ...,     3,     3,     3],\n",
      "        [    1, 22505, 10830,  ...,     3,     3,     3],\n",
      "        [    1, 16938, 14836,  ...,     3,     3,     3]])\n",
      "Decoder_ids: </s> 그렇다면 왕은 수렵이라도 가고 궁전만은 비어 있는 것일까 하고 돌축을 하나하나 밟아 가면 또다시 기다란 줄 행랑이 축을 하나하나 밟아 가면 또다시 기다란 줄 행랑이 있는 것이고, 그것을 오른편으로 돌아들어 왼편으로 보이는 별실은 천국 인 듯 조용한 목에 뜰 앞에 조롱들 속에서 빛깔 다른 새들이 시스마금 낯선 손님을 맞아 아는 체하고 재재거린다.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Labels IDs: tensor([[    1, 18769, 14949,  ...,     3,     3,     3],\n",
      "        [    1, 14306, 10897,  ...,     3,     3,     3],\n",
      "        [    1, 22505, 10830,  ...,     3,     3,     3],\n",
      "        [    1, 16938, 14836,  ...,     3,     3,     3]])\n",
      "Labels IDs: </s> 그렇다면 왕은 수렵이라도 가고 궁전만은 비어 있는 것일까 하고 돌축을 하나하나 밟아 가면 또다시 기다란 줄 행랑이 축을 하나하나 밟아 가면 또다시 기다란 줄 행랑이 있는 것이고, 그것을 오른편으로 돌아들어 왼편으로 보이는 별실은 천국 인 듯 조용한 목에 뜰 앞에 조롱들 속에서 빛깔 다른 새들이 시스마금 낯선 손님을 맞아 아는 체하고 재재거린다.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(loader):\n",
    "    token_ids, decoder_ids, labels_ids = batch\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    print(len(token_ids[0]))\n",
    "    print(token_ids.shape)\n",
    "    print(token_ids[0][7])\n",
    "    print(tokenizer.decode(token_ids[0][7]))\n",
    "    print(tokenizer.decode(token_ids[0]))\n",
    "\n",
    "    print(len(token_ids))\n",
    "    print(\"Decoder_ids:\", decoder_ids)\n",
    "    print(\"Decoder_ids:\", (tokenizer.decode(decoder_ids[0])))\n",
    "\n",
    "    print(\"Labels IDs:\", labels_ids)\n",
    "    print(\"Labels IDs:\", (tokenizer.decode(labels_ids[0])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247eb455319a4587a2c3b4520b234619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|          | 0/14545 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델과 옵티마이저 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 스케쥴러 설정\n",
    "epochs = 15\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(loader)*epochs)\n",
    "\n",
    "# 학습 루프\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(loader, desc=\"Epoch {:1d}\".format(epoch+1), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, decoder_input_ids, labels = batch\n",
    "\n",
    "        # 모델 forward 및 loss 계산\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 백워드 및 옵티마이저 스텝\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
    "\n",
    "    print(\"Epoch {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, total_loss / len(loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Similar_bart_model_05_31_last/tokenizer_config.json',\n",
       " './Similar_bart_model_05_31_last/special_tokens_map.json',\n",
       " './Similar_bart_model_05_31_last/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./Similar_bart_model_05_31_last\")\n",
    "tokenizer.save_pretrained(\"./Similar_bart_model_05_31_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: 나는 이 잔디밭에 누어서 소년 다운 온갖 공상을 푸른 하늘로 날개처럼 펼치었었소.\n",
      "생성 1: 나는 이 잔디밭에 누어서 소년 다운 온갖 공상을 푸른 하늘로 날개처럼 펼치었었소..었다.습니다. 였요.이었다소 말았으니하였이지말정말로언 언 하였정이말이 말이언이언은 말은은 말을 은 말도도 뜻밖이 소리소리가소리 소리가 비비 비가비가비는비를며비와비,비고비고비고비고고글 글 문 글을 써 편지를 써서 글씨 편지 속에 넣어에 넣고 몸에 걸 데니 주머니주머니엔에는에도 들어 바까지 든든들어속속에 들어가 들어갔속으로 들어간다.이다.\n",
      "생성 2: 나는 이 잔디밭에 누어서 소년 다운 온갖 공상을 푸른 하늘로 날개처럼 펼치었었소.습니다.았 놓쳤습니다니렸으니 다행이지.지....... 소용 \" 그러나 그것도 또 이것도것도 그렇지만도 이건 것도 그렇 아니라 황 란 역시 그러 같은 건이란 말 이란 말은란 뜻밖의 말에 앞이 아뜩해지며 아무 말도 할 수가 없더구나? 도무지할 수 없이 식 없는 일 인 충동을동이동 동무를 하고 어이가 있!무가 있는 하는 것 같 보인가요어렴이는하는 이는 누구 시인은 사람은 것은 사람일인 듯싶은갑는 구름가는사람들 은가 자꾸만 아는 사람이 먹는 명 저가, 맛 는다는 좋다는 소문을 듣고 마음에 남의 것을 좋은 좋지 좋고 좋 싫좋 생각이 제일 좋다고 좋을 가장 좋아야겠다는 생각 그 것이 좋아하는 싶은 것일 싶 것인데 것이지 지금 처럼 요즘 우리 것만 가지고 오늘이라도 이것저것을 위해서 나 옛날까 먼저 생긴 것이니 예전라도 이런 데까지 오늘날 우리나라 일본이나 조선, 일본, 나라 것까지 일본으로 해서 무슨지,분 하고, 다른 나라에 분 여러 나라로 않고,나라에하 고 하지와해서 척척시켰다시피부터 놈했하겠거니와뿐입니다.에,한으로,석 애초부터반애당초 당초하였 사러 원수보다도 것입니다.하여 이와연 알에서즉 방금 오직으로 한으로서 책임물 정하였던 바발 발랄 하여튼히 무엇으므로미행하였다.]으로써 미 육 불난 까닭 장차 위하여 이미 말한 사실혼으로서의진대 위에 대로서 무어 야 모적으로든 실 말로써 진 삼 아니라, 함게로 해결짓말로 그대로 표현하면, 그것이다. 것이다. 즉적인 질 다 형용되었다.얼리 중단으로 된서된없이소 애매 미상불 해 막연히 대답스러운스럽게 되었다. 듯이 영리한이었다. 마치 잡 여전히 없었다.없는 가쁜였다.으로는 성 만연한 가운데 다만 영구 괴인들 얼짐없었던 거짓말 퍼쩍대고사를하게성 안 모양 허둥 떠서 듯한함이뚱 아닌 혹시 떠 됨 성가 난 설에감에라, 아니었다. 또는 연 나섰 없었던 일이 응 가만 히 무엇을 다시만에슬렸다.름하고 이상한하게도았다.에서는 못마 이해 의미로했던 거스럭거리렀다. 기회에 갑자기 몰렸던회가 점점 더 갔 않아서 몹시 크게놈 떼까? 덥른뜬떡 하나 이윽고에는리는 않았다.심 괴로 그저 캄했다.뜻 마음 지긋거 얄 가벼웠 참아 끝컴 깜박 미소 피자 않게 하려 덮치는심과 감격려니와 뜨겁고 무서운 조금도 불안 일으켰더니 정말 동안 좀 감고 일으키려고 머리털 쾌활한 태도로 조금 느끼졌다.들이 쪽에서 드 서\n",
      "생성 3: 나는 이 잔디밭에 누어서 소년 다운 온갖 공상을 푸른 하늘로 날개처럼 펼치었었소..았으니  좋다 좋은 다행이지지겠 있겠지도 않겠느냐던 내 생각도 것도 것 같으 들었다.았다.이었다.이었였였다.였던이었던었던았던 것이었 말이었을 것이다.이다.\n",
      "생성 4: 나는 이 잔디밭에 누어서 소년 다운 온갖 공상을 푸른 하늘로 날개처럼 펼치었었소.습니다.었다. 것이었다이었으니 되었습니니였니까었을겠었던았던던였던이었던 했던졌던 되었던 있었던 없었던 있던 기억 추억 고향의 옛 고향 산속의속 속속을속에 속을 속에속으로속에서 속에서 속으로 들어가 들어갔다. 들어간든 든나 방방 방으로 방에방으로방은방의방이방에방과실은실이실과 부엌실을실 문 전실의 제법 법리하 뭇사람들의 눈에 간혹쑥 낯선 선뜻한과의에 찬 눈 의연한을도를 물 만 리 스치고 치고수에수를수와수가수의수는 모두 수는 수와 수가 수숫수만 숫자 숫자를 숫 사람 수로 수에 숫자로 보면은 사람이 그 숫자가 실 모양보다 더 열 저 꼴이 추 순 가히 셈하게 되으리 것이하리라. 것이다.이다. “ 되었다. ‘’ 된다.군 되는 ) 될 것 군! 됩니다. 된서된서는 되지 않은 서 되어서면 되기 난 되어 받아 가지고 못한 되면다면이라도서를 쓰지 못하게 할 못하도록 않을 쓰게 아니 물론할아 아난요? 요세요.요, 세 것은 아니지만. 아니고 아직 없는 보 아니라 쓴이라기보다는 쓰는 아닌 게 사용 후...가까 보다 것을지 말입니다.에는 [] 한 하는 아닐,까지는 하나 지금 상태 그저 그렇 그것보다는 무서운 하고는보다도 더욱적이 점점 이제 막 불 이번에는 좀 이상한 하니 그러면 다시 예전 조금 나 옛날 오히려 발 것처럼 하면 일 위력자 철퍽하니진 도리어 역 딴 생각 것도 못 생긴럼 또 다른 한편으로는 자꾸 와 하더니 글자도 있 그럴듯 여러분하고에서는 영심이서도 일도 혹시 따돌았다.석동기도와심과 온도 또한 역시 자기에게는 성 스며로의 훨씬 전부터 한참 동안이었다.성까지효가 영구 없었다.영웅 만큼씩발연동을 들 끝 설으므로스레똘 한결 생기 시작하였다. 봉우 별도 없이 석양반의였다.장의 자리에양의 마음장해시의 위치를 감 숨감을 봉인 양 잠시 있을뿐 자기의 움큼 성의만은만에불을 드높은자리에 풀 힘껏올라 얼굴 가득 뜨겁고 책임 머리 위에 올라앉아, 등허둥 떠서 놀 듯이 덥털 쾌 얼하여만으로 솜 같은즉 솟서서으로는지도 틀어막연히흥얼렸다. 돌단에숨뜩해지며으되니,없이 물러앉아 운 지긋가,놓아서 빙 얼어붙고 바라 뚜렷이 들려오는 듯 덮쳐 다만 거세게이라리며 떠 소리 있게 해 질겁 달려 우러스르 떨리는지는질 놓 까닭에,라,며, 드문드문 하여 가만 무슨감에라 하였다. 하려니와치며서, 없 고 고개를대고똑 쳐들고 똑쩍\n",
      "생성 5: 나는 이 잔디밭에 누어서 소년 다운 온갖 공상을 푸른 하늘로 날개처럼 펼치었었소..습니다.쳤  않았다 하지 않 했 못 못했 못한하지 않은지 못하고 못하는 못할 오늘 오늘의 오늘에 내 지금 나의 아직 현재 있는 그대로의 상태 그대로 모습 것 대로대로 따라 따라서다면 그 그것 하면 되는 하는 것이하는 그것이 생각하는 것은 생각 것도 일 사실인 듯하였다.하였하였으나하던하였던 같았으나하여 어찌 이와 같은 와과음 과한 음흉 흉내를 난 상처를낸 낸 고인을정한 글을 쓴 을 작자의 필 한 분 양을만한 만한 애 가진자를 가지지 놈을 둔 두고 있 계집 놓고는를 하 이렇게 해서밑 밑 가 저렇게 된 나 안준다.가선 것이다. 선 더 큰에나 준큰 데 작은씨푼데저하난 푼른 호주머니 주머니 싼 든 커다란에다 손을대고 손 마른 헝겊 동미를 싸놓은 써보이어 또 입의으로 만든 책상 걸본이다.고 말한 말 그리고시의 구절 있다. 『자 자막에』 라고 말하고 싶은 하고싶 싶 더욱 마음 풀리고 싶어 올 할만 생각이 한다.려고 생각하고 있는데 다시 생각은 없 그럴 것만 같아서 다만 그러려니 모든 것을 모두 대답할 뿐이지연 수가 아니고 대 수만은 아니 물론까지뿐 무슨 지 연 알 수는 없는 그렇지, 사랑하는 것이지마는 모두가 은, 아무것도 모르는 바까 도대체 무엇이니,이라도, 라도 역시 하나도 다 같이애 아니라 다른 누구이고 우리 서로 친구가 아닌가, 나와 같지 여러 아 딸들이며들끼리 이야기라서라기보다는 친구 서러과는 형제 자매우들과 함께와 사이스럼 우수와 영우와 같아 혹간더러와는 얼근히냐 아니면 자기보다 훨씬 보다 둘?아! 둘이 무엇하겠느냐 그저 싱거운낄껄 웃이라도 따돌았던 어머니요까? 어떨 것인가소 눈동다보 간 보인가 빙글거기만 허브 거구, 좋아 그리라도 모양 얼굴로 않게 아주 영구하고 즐겁고 편하게놈 화 질질얼린기 것처럼 둥 떠 하면서 자연스럽게스럽게뚱거리는 듯한함이 오히려스러운똘 끝없이석허둥 뜨렸다. 돌았다. 놀랜 듯이 덥썩해리며 신기했다.그스레 미소 놓 쳐들고 가만하니 불안함 떨었다. 머그레 응스러웠 없었다. 동안만에불을 들리는 소리 석 번뜩해지기도기는무섭은심과 불쾌한지는 잠시슬쩍 함박 치며 보기에 무심히 고개를 흔들리 않았다.마 해봤더니 일이연히이었다.롬라. \"뜻심이 들어라, “...”.......... 가벼 마음이같아,니다.심한판을 벌떡쇠불 똑 하나 이윽고맞 원 귀여─ 자기의 발길을 좀\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,PreTrainedTokenizerFast,AutoModelForCausalLM, AutoTokenizer\n",
    "def generate_sequence(context, model, tokenizer, device, num_sequences=2):\n",
    "      # 모델을 평가 모드로 설정\n",
    "\n",
    "    # 입력 문맥 토크나이징\n",
    "    input_ids = tokenizer.encode(BOS + context + EOS, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 모델을 사용하여 출력 생성\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=512,\n",
    "        temperature=0.7,\n",
    "        top_k=5,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=2.0,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_sequences\n",
    "    )\n",
    "\n",
    "    generated_sequences = []\n",
    "    for generated_sequence in output_sequences:\n",
    "        # 생성된 시퀀스를 디코딩하여 텍스트로 변환\n",
    "        decoded_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        generated_sequences.append(decoded_text)\n",
    "    \n",
    "    return generated_sequences\n",
    "\n",
    "model_path = \"./\"+\"Similar_gpt2_model1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def multiple_replace(text, replacements):\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "replacements = {BOS: \"\", EOS: \"\", PAD: \"\"}\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "context = test_data[\"context\"][3]\n",
    "print(\"원본:\", context)\n",
    "\n",
    "# context를 input으로 넣으면 context +question +answer 형태로 만듬\n",
    "generated_sentences = generate_sequence(context, model, tokenizer, device, num_sequences=5)\n",
    "\n",
    "for idx, sentence in enumerate(generated_sentences):\n",
    "    sentence = multiple_replace(sentence, replacements)\n",
    "    print(f\"생성 {idx+1}:\", sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.34.0\n",
      "Uninstalling transformers-4.34.0:\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 816, in move\n",
      "    os.rename(src, real_dst)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/local/bin/transformers-cli' -> '/tmp/pip-uninstall-rsim9_ne/transformers-cli'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/uninstall.py\", line 105, in run\n",
      "    uninstall_pathset = req.uninstall(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 687, in uninstall\n",
      "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 381, in remove\n",
      "    moved.stash(path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 272, in stash\n",
      "    renames(path, new_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 315, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 837, in move\n",
      "    os.unlink(src)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/local/bin/transformers-cli'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-8uzp1br7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-8uzp1br7\n",
      "  Resolved https://github.com/huggingface/transformers to commit d19566e8523547d833d674d973908dae569dfba4\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (3.11.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.42.0.dev0)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/78/71/6ce4136149cb42b98599d49c39b3a39dd6858b5f9307490998c40e26a51e/huggingface_hub-0.23.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.42.0.dev0)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.42.0.dev0)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/8f/05/969e1a976b84283285181b00028cf73d78434b77a6627fc2a94194cca265/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.42.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.42.0.dev0) (2023.7.22)\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.42.0.dev0-py3-none-any.whl size=9131493 sha256=f0926efe1fbe36ba9476b8686cadb0f2c40cb8391af074110b8494a1d5736cc2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3s2z4w1e/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.2 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.0.dev0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y transformers\n",
    "%pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da78407d385f4cc497e28f5bd096b297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSimilar_phi3_model1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[39m=\u001b[39m  AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_path)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m Similar_data_test\u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m:[]}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,PreTrainedTokenizerFast,AutoModelForCausalLM, AutoTokenizer\n",
    "model_path = \"./\"+\"Similar_phi3_model1\"\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "Similar_data_test= {\"data\":[]}\n",
    "Similar_data_train= {\"data\":[]}\n",
    "def Similar_generation(Similar_data, context ,model,tokenizer,device):\n",
    "    para = {\"para\":[]}\n",
    "    sequence = {\"sequence\":[]}\n",
    "    sequence[\"context\"] = context\n",
    "     \n",
    "    generated_sentences = generate_sequence(context, model, tokenizer, device, num_sequences=2)\n",
    "    for idx, sentence in enumerate(generated_sentences):\n",
    "        sentence = multiple_replace(sentence, replacements)\n",
    "        sequence[\"sequence\"].append(sentence)\n",
    "\n",
    "    para[\"para\"].append(sequence)\n",
    "    Similar_data[\"data\"].append(para)\n",
    "\n",
    "    return Similar_data\n",
    "    \n",
    "\n",
    "\n",
    "for i, data in tqdm(enumerate(test_data[\"context\"][:1000]), total=test_data[\"context\"][:1000].shape[0]):\n",
    "    Similar_generation(Similar_data_test, data, model, tokenizer, device)\n",
    "\n",
    "with open(model_path+'_testdata.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_test, file, indent='\\t', ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 12.00it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, data in tqdm(enumerate(train_data[\"context\"][:1000]), total=train_data[\"context\"][:1000].shape[0]):\n",
    "    Similar_generation(Similar_data_train, data, model, tokenizer, device)\n",
    "    \n",
    "with open(model_path+'_traindata.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_train, file, indent='\\t', ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
