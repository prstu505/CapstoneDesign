{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
      "\u001b[33mDEPRECATION: devscripts 2.22.1ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of devscripts or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처럼의 갯수: 237\n",
      "같은의 갯수: 59\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data_phrase/pattern1_phrase.json\", 'rb') as f :\n",
    "    data1 = json.load(f)\n",
    "with open(\"./data_phrase/pattern2_phrase.json\", 'rb') as f :\n",
    "    data2 = json.load(f)\n",
    "sentence_token = []\n",
    "\n",
    "count = 0\n",
    "for info in enumerate(data1): \n",
    "    para = {}\n",
    "    form = info[1][0]\n",
    "    para[\"sentence\"] = info[1][0]\n",
    "    para[\"auxiliary_idea\"] =info[1][1][0][\"auxiliary_idea\"]\n",
    "    if para[\"auxiliary_idea\"] == \"처럼\":\n",
    "        count+=1\n",
    "        continue\n",
    "    form_replace = form.replace(para[\"auxiliary_idea\"], \"[MASK]\")\n",
    "    para[\"mask_sentence\"] = form_replace\n",
    "    sentence_token.append(para)\n",
    "\n",
    "print(\"처럼의 갯수:\",count)\n",
    "\n",
    "count = 0\n",
    "for info in enumerate(data2): \n",
    "    para = {}\n",
    "    form = info[1][0]\n",
    "    para[\"sentence\"] = info[1][0]\n",
    "    para[\"auxiliary_idea\"] =info[1][1][0][\"auxiliary_idea\"]\n",
    "    if para[\"auxiliary_idea\"] == \"같은\":\n",
    "        count+=1\n",
    "        continue\n",
    "    form_replace = form.replace(para[\"auxiliary_idea\"], \"[MASK]\")\n",
    "    para[\"mask_sentence\"] = form_replace\n",
    "    sentence_token.append(para)\n",
    "\n",
    "print(\"같은의 갯수:\",count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4331\n",
      "지나인 가는 곳의 포목상, 일본인 가는 곳의 창부처럼, 서양인 가는 곳의 앞서는 이는 언제든지 선교사이니까, 조선에 맨 먼저 들어온 서양인 급 서양 문물도 기독교의 일파로, 그때에 흔히 서학, 혹 천주학 내지 사학의 그것이었습니다.\n",
      "일본인 가는 곳의 창부처럼\n",
      "지나인 가는 곳의 포목상, [MASK], 서양인 가는 곳의 앞서는 이는 언제든지 선교사이니까, 조선에 맨 먼저 들어온 서양인 급 서양 문물도 기독교의 일파로, 그때에 흔히 서학, 혹 천주학 내지 사학의 그것이었습니다.\n",
      "당시 조선의 「개」씨왕조에서는 선진국인 지나로 더불어 교제를 함에나, 또 국내에 있는 지나 귀화민을 통치해 감에나, 다 지나인에게 외이라는 모시를 당 함보다는, 지나 고대의 성인적를 조선으로 함이 심 히 필요하므로, 이 신설을 즐겨 채용하여 스스로 기자의 자손으로 처하고, 이 신설이 뒤에 지나의 본토로도 역수입되어, 사마천의 〈사기〉와 같은 문헌에도 기자가 조선에 군림한 것처럼 기재 하매 나중에는 본연한 해성은 이럭저럭 소실되고, 지나의 문적에 등재해 있는 기자의 가모적 사실한 신인을 얻게 되었다.\n",
      "것처럼\n",
      "당시 조선의 「개」씨왕조에서는 선진국인 지나로 더불어 교제를 함에나, 또 국내에 있는 지나 귀화민을 통치해 감에나, 다 지나인에게 외이라는 모시를 당 함보다는, 지나 고대의 성인적를 조선으로 함이 심 히 필요하므로, 이 신설을 즐겨 채용하여 스스로 기자의 자손으로 처하고, 이 신설이 뒤에 지나의 본토로도 역수입되어, 사마천의 〈사기〉와 같은 문헌에도 기자가 조선에 군림한 [MASK] 기재 하매 나중에는 본연한 해성은 이럭저럭 소실되고, 지나의 문적에 등재해 있는 기자의 가모적 사실한 신인을 얻게 되었다.\n",
      " 두고 센 나룻을 뽑히는데, 첩은 조심조심하여 센 털만 골라서 뽑건마는, 본처는 첩에게 젊게 보이려 드는 것이 미워서 검은 털을 뽑으매, 얼마 뒤에는 검은 털 센 털이 말끔 없어져서 늙은 마누라처럼 된 고로, 창피하여서 오랫동안 나들이를 못하였다는 이야기.\n",
      "없어져서 늙은 마누라처럼\n",
      " 두고 센 나룻을 뽑히는데, 첩은 조심조심하여 센 털만 골라서 뽑건마는, 본처는 첩에게 젊게 보이려 드는 것이 미워서 검은 털을 뽑으매, 얼마 뒤에는 검은 털 센 털이 말끔 [MASK] 된 고로, 창피하여서 오랫동안 나들이를 못하였다는 이야기.\n"
     ]
    }
   ],
   "source": [
    "print(len(sentence_token))\n",
    "print(sentence_token[1386][\"sentence\"])\n",
    "print(sentence_token[1386][\"auxiliary_idea\"])\n",
    "print(sentence_token[1386][\"mask_sentence\"])\n",
    "print(sentence_token[1387][\"sentence\"])\n",
    "print(sentence_token[1387][\"auxiliary_idea\"])\n",
    "print(sentence_token[1387][\"mask_sentence\"])\n",
    "print(sentence_token[1388][\"sentence\"])\n",
    "print(sentence_token[1388][\"auxiliary_idea\"])\n",
    "print(sentence_token[1388][\"mask_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "KoELECTRA: {'score': 0.3041920065879822, 'token': 4034, 'token_str': '##는', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에는 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.03953099250793457, 'token': 2126, 'token_str': '그', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 그 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.03691812977194786, 'token': 7549, 'token_str': '갑자기', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 갑자기 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.026266777887940407, 'token': 13351, 'token_str': '문득', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 문득 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.021406855434179306, 'token': 4474, 'token_str': '##야', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에야 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.01627970114350319, 'token': 2252, 'token_str': '내', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 내 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.014457706362009048, 'token': 6453, 'token_str': '그녀', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 그녀 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.014100641943514347, 'token': 7319, 'token_str': '나타나', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 나타나 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.013108331710100174, 'token': 16, 'token_str': ',', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에, 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.012401431798934937, 'token': 7225, 'token_str': '그만', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 그만 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.011300066486001015, 'token': 8566, 'token_str': '벌써', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 벌써 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.011130020953714848, 'token': 4129, 'token_str': '##서', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에서 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.011123506352305412, 'token': 6881, 'token_str': '어머니', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 어머니 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.00841077696532011, 'token': 7417, 'token_str': '나의', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 나의 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.007669639773666859, 'token': 6304, 'token_str': '다시', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 다시 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.007608684245496988, 'token': 4065, 'token_str': '##나', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에나 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.007567739579826593, 'token': 24398, 'token_str': '떠올라', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 떠올라 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.007556798867881298, 'token': 7374, 'token_str': '일어나', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 일어나 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.006282117683440447, 'token': 15509, 'token_str': '찾아와', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 찾아와 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.006224545184522867, 'token': 7027, 'token_str': '나와', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 나와 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.0061529334634542465, 'token': 20996, 'token_str': '불쑥', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 불쑥 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.006108755245804787, 'token': 22219, 'token_str': '##서야', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에서야 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.005735957995057106, 'token': 7982, 'token_str': '와서', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 와서 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.0049448320642113686, 'token': 6683, 'token_str': '얼굴', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 얼굴 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.004913299810141325, 'token': 7406, 'token_str': '아내', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 아내 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.004464225377887487, 'token': 11001, 'token_str': '돌아와', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 돌아와 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.004366148728877306, 'token': 8518, 'token_str': '##라도', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에라도 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.004188683349639177, 'token': 13830, 'token_str': '##인지', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에인지 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.0037725036963820457, 'token': 3860, 'token_str': '흰', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 흰 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "KoELECTRA: {'score': 0.0037643813993781805, 'token': 14600, 'token_str': '돌연', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 돌연 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.21032080054283142, 'token': 636, 'token_str': '그', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 그 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.05191575735807419, 'token': 1, 'token_str': '[PAD]', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.05166923627257347, 'token': 16, 'token_str': ',', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에, 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.03929446265101433, 'token': 13969, 'token_str': '고운', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 고운 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.03916860371828079, 'token': 864, 'token_str': '두', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 두 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.03573668748140335, 'token': 8939, 'token_str': '어느새', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 어느새 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.023598171770572662, 'token': 7720, 'token_str': '하얀', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 하얀 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.023393258452415466, 'token': 1968, 'token_str': '흰', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 흰 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.016007542610168457, 'token': 732, 'token_str': '내', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 내 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.015718691051006317, 'token': 12918, 'token_str': '어느덧', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 어느덧 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.015532615594565868, 'token': 1545, 'token_str': '제', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 제 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.012691190466284752, 'token': 5442, 'token_str': '벌써', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 벌써 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.011184747330844402, 'token': 28709, 'token_str': '메마른', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 메마른 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.011174500919878483, 'token': 15207, 'token_str': '둥근', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 둥근 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.010857214219868183, 'token': 4416, 'token_str': '그만', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 그만 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.00965898297727108, 'token': 7219, 'token_str': '푸른', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 푸른 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.00934928935021162, 'token': 4766, 'token_str': '갑자기', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 갑자기 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.00892755389213562, 'token': 29383, 'token_str': '여린', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 여린 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.008513404056429863, 'token': 1445, 'token_str': '온', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 온 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.008005446754395962, 'token': 11207, 'token_str': '지친', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 지친 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.007951046340167522, 'token': 5212, 'token_str': '아름다운', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 아름다운 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.00756635470315814, 'token': 19943, 'token_str': '까만', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 까만 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.007525903172791004, 'token': 5241, 'token_str': '와서', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 와서 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.007004357408732176, 'token': 8146, 'token_str': '부드러운', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 부드러운 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.006567315198481083, 'token': 646, 'token_str': '긴', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 긴 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.006132384296506643, 'token': 8831, 'token_str': '문득', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 문득 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.005860944278538227, 'token': 10853, 'token_str': '마른', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 마른 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.005833536386489868, 'token': 6282, 'token_str': '커다란', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 커다란 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.005297224968671799, 'token': 1535, 'token_str': '저', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 저 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n",
      "klue_large: {'score': 0.0051636407151818275, 'token': 4030, 'token_str': '어린', 'sequence': '내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 어린 두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.'}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "KoELECTRA = \"monologg/koelectra-base-v3-generator\"\n",
    "klue_base = \"klue/bert-base\"\n",
    "klue_large = \"klue/roberta-large\"\n",
    "large =\"KoichiYasuoka/roberta-large-korean-hanja\"\n",
    "multilingual ='bert-base-multilingual-uncased'\n",
    "k = 30\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(KoELECTRA)\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(klue_large)\n",
    "# tokenizer2 = AutoTokenizer.from_pretrained(klue_base)\n",
    "# tokenizer4 = AutoTokenizer.from_pretrained(large)\n",
    "# tokenizer5 = AutoTokenizer.from_pretrained(multilingual)\n",
    "\n",
    "\n",
    "\n",
    "KoELECTRA_pip = pipeline(task ='fill-mask',top_k = k, model=KoELECTRA,tokenizer=tokenizer1, device= device)\n",
    "klue_large_pip = pipeline(task ='fill-mask',top_k = k, model=klue_large,tokenizer=tokenizer3, device= device)\n",
    "# klue_base_pip = pipeline(task ='fill-mask',top_k = k, model=klue_base,tokenizer=tokenizer2, device= device)\n",
    "# large_pip = pipeline(task ='fill-mask',top_k = k, model=large,tokenizer=tokenizer4, device= device)\n",
    "# multilingual_pip = pipeline(task ='fill-mask',top_k = k, model=multilingual,tokenizer=tokenizer5, device= device)\n",
    "\n",
    "text = \"내게 시집올 때에는 방글방글 피려는 들꽃 봉오리 같던 아내가 어느 결에 [MASK]두 뺨에 선연 한 빛이 스러지고 이마에는 벌써 두어 금 가는 줄이 그리어졌다.\"\n",
    "test1 =KoELECTRA_pip(text)\n",
    "test4 =klue_large_pip(text)\n",
    "# test3 =klue_base_pip(text)\n",
    "# test5 =large_pip(text)\n",
    "# test6 =multilingual_pip(text)\n",
    "\n",
    "for i in enumerate(test1):\n",
    "    print('KoELECTRA:',i[1])\n",
    "\n",
    "for i in enumerate(test4):\n",
    "    print('klue_large:',i[1])\n",
    "# for i in enumerate(test3):\n",
    "#     print('klue_base:',i[1])\n",
    "# for i in enumerate(test5):\n",
    "#     print('large:',i[1])\n",
    "# for i in enumerate(test6):\n",
    "#     print('multilingual:',i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "dict = {\"data\": [{\"para\": [{\"label\": [{}], \"context\": \"\"}]}]}\n",
    "\n",
    "def multiple_replace(text):\n",
    "    replacements = {\"[CLS]\": \"\", \"[SEP]\": \"\"}\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# 필터링 함수\n",
    "def filter_tokens(predictions, original_token_str,top_k=5 ):\n",
    "    filtered_tokens = []\n",
    "    exclude_tokens = {\"[UNK]\", \"’\",\".\",\"’’\", \"'\",\"''\", \",,\",\",\", \"(\", \")\",\"<\",\">\" ,\"~\",\"[PAD]\",\"/\",\"~\",\"』\",\"\\\\\",\"?\",\"!\",\"\",\"-\",\"―\",\"<<\",\">>\",\"\\\"\",\"...\",\"…\"}\n",
    "    \n",
    "    context_label_pairs =[]\n",
    "    count = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        labels = {}\n",
    "        if str(type(pred)) ==\"<class 'list'>\":\n",
    "            for sequence_list in pred:\n",
    "                label = sequence_list[\"sequence\"]\n",
    "                token_str = sequence_list[\"token_str\"] \n",
    "                label = label.replace(\"[MASK]\",token_str)\n",
    "                label = multiple_replace(label)\n",
    "                labels[\"sequence\"] = label\n",
    "                labels[\"token_str\"] = token_str\n",
    "                labels[\"count\"] = count\n",
    "                context_label_pairs.append(labels)\n",
    "                count+=1\n",
    "        else:\n",
    "            label = pred[\"sequence\"]\n",
    "            token_str = pred[\"token_str\"] \n",
    "            label =label.replace(\"[MASK]\",token_str)\n",
    "            label = multiple_replace(label)\n",
    "            labels[\"sequence\"] = label\n",
    "            labels[\"token_str\"] = token_str\n",
    "            labels[\"count\"] = count\n",
    "            context_label_pairs.append(labels)\n",
    "            count+=1\n",
    "    \n",
    "    prev_token = \"\"\n",
    "\n",
    "    for pred in context_label_pairs:\n",
    "        token_str = pred[\"token_str\"]\n",
    "        if token_str != prev_token and token_str != original_token_str and (token_str not in exclude_tokens and not token_str.startswith(\"##\")):\n",
    "            prev_token = token_str\n",
    "            filtered_tokens.append(pred)\n",
    "            \n",
    "        elif token_str == prev_token:\n",
    "            continue\n",
    "        if len(filtered_tokens) >= top_k:\n",
    "            break\n",
    "    return filtered_tokens\n",
    "\n",
    "def generation_label (pipes,tokenizer,sentence_token):\n",
    "    Similar_data = {\"data\": []}\n",
    "    for sentence in tqdm(sentence_token, total=len(sentence_token)):\n",
    "        labels = {\"label\": []}\n",
    "        para = {\"para\": []}\n",
    "        len_token = tokenizer.tokenize(sentence[\"mask_sentence\"])\n",
    "        #입력 길이 초과 처리\n",
    "        if len(len_token) >= 512:\n",
    "            len_token = len_token[:500]\n",
    "\n",
    "        context = tokenizer.convert_tokens_to_string(len_token)\n",
    "        labels[\"context\"] = sentence[\"sentence\"]\n",
    "        labels[\"auxiliary_idea\"] = sentence[\"auxiliary_idea\"]\n",
    "        raw_predictions = pipes(context)\n",
    "        filtered_predictions = filter_tokens(raw_predictions,sentence[\"auxiliary_idea\"] ,top_k=5 )\n",
    "        labels[\"label\"].extend(filtered_predictions)\n",
    "\n",
    "        para[\"para\"].append(labels)\n",
    "        Similar_data[\"data\"].append(para)\n",
    "\n",
    "    return Similar_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4331 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 4331/4331 [00:47<00:00, 90.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: {'para': [{'label': [{'sequence': '쓸쓸스러운 붉은 감 닙이 바람 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '바람', 'count': 6}, {'sequence': '쓸쓸스러운 붉은 감 닙이 그 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '그', 'count': 8}, {'sequence': '쓸쓸스러운 붉은 감 닙이 날 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '날', 'count': 15}, {'sequence': '쓸쓸스러운 붉은 감 닙이 나 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '나', 'count': 16}, {'sequence': '쓸쓸스러운 붉은 감 닙이 내 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '내', 'count': 20}], 'context': '쓸쓸스러운 붉은 감 닙이 죽어가는 생물처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'auxiliary_idea': '죽어가는 생물처럼'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Similar_data_KoELECTRA = generation_label (KoELECTRA_pip,tokenizer1,sentence_token)\n",
    "print(\"3:\",Similar_data_KoELECTRA[\"data\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_label_test_bert/Similar_KoELECTRA_phrase_data.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_KoELECTRA,file, indent='\\t', ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4331/4331 [01:16<00:00, 56.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: {'para': [{'label': [{'sequence': '쓸쓸스러운 붉은 감 닙이 세상 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '세상', 'count': 1}, {'sequence': '쓸쓸스러운 붉은 감 닙이 가슴 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '가슴', 'count': 2}, {'sequence': '쓸쓸스러운 붉은 감 닙이 하늘 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '하늘', 'count': 3}, {'sequence': '쓸쓸스러운 붉은 감 닙이 마음 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '마음', 'count': 5}, {'sequence': '쓸쓸스러운 붉은 감 닙이 바람 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '바람', 'count': 6}], 'context': '쓸쓸스러운 붉은 감 닙이 죽어가는 생물처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'auxiliary_idea': '죽어가는 생물처럼'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Similar_data_klue_large = generation_label (klue_large_pip,tokenizer3,sentence_token)\n",
    "print(\"2:\",Similar_data_klue_large[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_label_test_bert/Similar_klue_large_phrase_data.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_klue_large,file, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2549/4042 [00:28<00:16, 92.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 4042/4042 [00:46<00:00, 87.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: {'para': [{'label': [{'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 것 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '것', 'count': 0}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 시체 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '시체', 'count': 1}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 벌레 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '벌레', 'count': 2}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 짐승 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '짐승', 'count': 3}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 갈대 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '갈대', 'count': 4}], 'context': '쓸쓸스러운 붉은 감 닙이 죽어가는 생물처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Similar_data_klue_base = generation_label (klue_base_pip,tokenizer2,sentence_token)\n",
    "print(\"1:\",Similar_data_klue_base[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_label_test_bert/Similar_klue_base_data.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_klue_base, file, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2556/4042 [00:50<00:28, 52.67it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 4042/4042 [01:18<00:00, 51.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: {'para': [{'label': [{'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 것 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '것', 'count': 0}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 나무 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '나무', 'count': 1}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 바람 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '바람', 'count': 2}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 나뭇가지 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '나뭇가지', 'count': 3}], 'context': '쓸쓸스러운 붉은 감 닙이 죽어가는 생물처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Similar_data_large = generation_label (large_pip,tokenizer4,sentence_token)\n",
    "print(\"4:\",Similar_data_large[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_label_test_bert/Similar_large_data.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_large, file, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4041/4041 [01:00<00:00, 67.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: {'para': [{'label': [{'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 한 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '한', 'count': 1}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 날 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '날', 'count': 2}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 이 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '이', 'count': 4}], 'context': '쓸쓸스러운 붉은 감 닙이 죽어가는 생물처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Similar_data_multilingual = generation_label (multilingual_pip,tokenizer5,sentence_token)\n",
    "print(\"5:\",Similar_data_multilingual[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_label_test_bert/Similar_multilingual_data.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_multilingual, file, indent='\\t', ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 28 14:33:00 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  CUDA GPU                       On  | 00000000:E3:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              73W / 300W |   6322MiB / 81074MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dict = {\"data\": [{\"para\": [{\"label\": [{}], \"context\": \"\"}]}]}\n",
    "\n",
    "def multiple_replace(text):\n",
    "    replacements = {\"[CLS]\": \"\", \"[SEP]\": \"\"}\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "# 필터링 함수\n",
    "def filter_tokens(predictions, original_token_str):\n",
    "    filtered_tokens = []\n",
    "    exclude_tokens = {\"[UNK]\", \"’\",\".\",\"’’\", \"'\",\"''\", \",,\",\",\", \"(\", \")\",\n",
    "    \"<\",\">\" ,\"~\",\"[PAD]\",\"/\",\"~\",\"』\",\"\\\\\",\"?\",\"!\",\"\",\"-\",\"―\",\"<<\",\">>\",\"\\\"\",\"…\",\"...\"}\n",
    "    \n",
    "    context_label_pairs =[]\n",
    "    count = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        labels = {}\n",
    "        if str(type(pred)) ==\"<class 'list'>\":\n",
    "            for sequence_list in pred:\n",
    "                label = sequence_list[\"sequence\"]\n",
    "                token_str = sequence_list[\"token_str\"] \n",
    "                label = label.replace(\"[MASK]\",token_str)\n",
    "                label = multiple_replace(label)\n",
    "                labels[\"sequence\"] = label\n",
    "                labels[\"token_str\"] = token_str\n",
    "                labels[\"count\"] = count\n",
    "                labels[\"score\"] = sequence_list[\"score\"]\n",
    "                context_label_pairs.append(labels)\n",
    "                count+=1\n",
    "        else:\n",
    "            label = pred[\"sequence\"]\n",
    "            token_str = pred[\"token_str\"] \n",
    "            label =label.replace(\"[MASK]\",token_str)\n",
    "            label = multiple_replace(label)\n",
    "            labels[\"sequence\"] = label\n",
    "            labels[\"token_str\"] = token_str\n",
    "            labels[\"count\"] = count\n",
    "            labels[\"score\"] = pred[\"score\"]\n",
    "            context_label_pairs.append(labels)\n",
    "            count+=1\n",
    "    \n",
    "    prev_token = \"\"\n",
    "\n",
    "    for pred in context_label_pairs:\n",
    "        token_str = pred[\"token_str\"]\n",
    "        prev_token = \"\"\n",
    "\n",
    "        if token_str != prev_token and token_str != original_token_str and (token_str not in exclude_tokens and not token_str.startswith(\"##\")):\n",
    "            prev_token = token_str\n",
    "            filtered_tokens.append(pred)\n",
    "            \n",
    "        elif token_str == prev_token:\n",
    "            continue\n",
    "        \n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def compare_to_score(pred,top_k =5):\n",
    "    scored_tokens = []\n",
    "\n",
    "    for sequence in pred:\n",
    "        scored_tokens.append(sequence)\n",
    "        if len(scored_tokens) >= top_k:\n",
    "            break   \n",
    "\n",
    "    return scored_tokens\n",
    "    \n",
    "\n",
    "def generation_label (pipe_1,pipe_2,tokenizer_1,tokenizer_2,sentence_token):\n",
    "    Similar_data = {\"data\": []}\n",
    "    for sentence in tqdm(sentence_token, total=len(sentence_token)):\n",
    "        labels = {\"label\": []}\n",
    "        para = {\"para\": []}\n",
    "        \n",
    "        len_token = tokenizer_1.tokenize(sentence[\"mask_sentence\"])\n",
    "        #입력 길이 초과 처리\n",
    "        if len(len_token) >= 512:\n",
    "            len_token = len_token[:500]\n",
    "\n",
    "        context = tokenizer_1.convert_tokens_to_string(len_token)\n",
    "\n",
    "        len_token = tokenizer_2.tokenize(context)\n",
    "        #입력 길이 초과 처리\n",
    "        if len(len_token) >= 512:\n",
    "            len_token = len_token[:500]\n",
    "\n",
    "        context = tokenizer_2.convert_tokens_to_string(len_token)\n",
    "\n",
    "        labels[\"context\"] = sentence[\"sentence\"]\n",
    "        raw_predictions_1 = pipe_1(context)\n",
    "        raw_predictions_2 = pipe_2(context)\n",
    "\n",
    "        filtered_predictions1 = filter_tokens(raw_predictions_1,sentence[\"auxiliary_idea\"] )\n",
    "        filtered_predictions2 = filter_tokens(raw_predictions_2,sentence[\"auxiliary_idea\"] )\n",
    "        filtered_predictions1.extend(filtered_predictions2)\n",
    "\n",
    "        filtered_predictions = sorted(filtered_predictions1, key=lambda x: x[\"score\"],reverse=True)\n",
    "\n",
    "        seen_texts = set()\n",
    "        unique_labels = []\n",
    "        \n",
    "        for label in filtered_predictions:\n",
    "            if label[\"token_str\"] not in seen_texts:\n",
    "                unique_labels.append(label)\n",
    "                seen_texts.add(label[\"token_str\"])\n",
    "        \n",
    "        scored_predictions =compare_to_score(unique_labels,top_k=5)\n",
    "\n",
    "        labels[\"label\"].extend(scored_predictions)\n",
    "        para[\"para\"].append(labels)\n",
    "        Similar_data[\"data\"].append(para)\n",
    "\n",
    "    return Similar_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2553/4041 [01:33<00:53, 27.61it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 4041/4041 [02:28<00:00, 27.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: {'para': [{'label': [{'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 것 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '것', 'count': 0, 'score': 0.4975310266017914}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 나무 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '나무', 'count': 1, 'score': 0.05964517593383789}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 낙엽 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '낙엽', 'count': 1, 'score': 0.04508940875530243}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 바람 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '바람', 'count': 2, 'score': 0.03820124641060829}, {'sequence': '쓸쓸스러운 붉은 감 닙이 죽어가는 나뭇가지 처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.', 'token_str': '나뭇가지', 'count': 3, 'score': 0.02793775498867035}], 'context': '쓸쓸스러운 붉은 감 닙이 죽어가는 생물처럼 여기저기 휘둘러서 헛날닐 제 말 없이 오는 가을바람이 따뜻한 나의 가슴을 근질이고 지내감에, 나도 모르는 쓸쓸한 비애가 나의 두 눈을 공연히 울고 싶게 하였다.'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Similar_data_klue_large_KoELECTRA = generation_label (klue_large_pip,KoELECTRA_pip,tokenizer1,tokenizer3,sentence_token)\n",
    "print(\"1:\",Similar_data_klue_base[\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_label_test_bert/Similar_data_klue_large_KoELECTRA.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(Similar_data_klue_large_KoELECTRA, file, indent='\\t', ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
