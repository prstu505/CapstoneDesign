{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall -y transformers\n",
    "#%pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType,prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7872b2316b5425aa49e3d45224bf498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[    1, 29871, 30860,   238,   161,   155, 29871, 31406, 31299, 30708,\n",
      "         29871, 31487, 31533,   240,   148,   159, 31680, 31286, 29871,   238,\n",
      "           182,   131,   240,   156,   155, 30944, 31457, 29871,   239,   134,\n",
      "           139, 30906,   239,   157,   183, 29871,   240,   148,   159, 31680,\n",
      "           239,   159,   191, 30906, 29871, 30709, 30889, 29871,   239,   144,\n",
      "           171, 29871, 30981, 31578, 31527,   636,    13,    13,  1678,   835,\n",
      "          2799,  4080, 29901,    13,   268,   239,   159,   164, 31262, 30393,\n",
      "         29871,   240,   160,   147,   238,   141,   147,   239,   163,   132,\n",
      "           240,   160,   147,   238,   141,   147,   239,   163,   132, 30944,\n",
      "         31136,   238,   164,   160, 29871,   240,   151,   191, 30906,   240,\n",
      "           153,   139, 31286, 29871,   238,   152,   143, 31826, 29871, 30852,\n",
      "         31262, 30393, 29871, 31354, 31225,   239,   181,   155,   238,   162,\n",
      "           191, 29871,   238,   170,   148, 31189,    13,    13,  1678,   835,\n",
      "         13291, 29901,    13,   268,   239,   159,   164, 31262, 30393, 29871,\n",
      "           240,   160,   147,   238,   141,   147,   239,   163,   132,   240,\n",
      "           160,   147,   238,   141,   147,   239,   163,   132, 30944, 31136,\n",
      "           238,   164,   160, 29871,   240,   151,   191, 30906,   240,   153,\n",
      "           139, 31286, 29871,   238,   152,   143, 31826, 29871, 30852, 31262,\n",
      "         30393, 29871,   238,   175,   191,   239,   181,   155,   238,   162,\n",
      "           191, 29871,   238,   170,   148, 31189]])\n",
      "Labels: tensor([[    1, 29871,   239,   159,   164, 31262, 30393, 29871,   240,   160,\n",
      "           147,   238,   141,   147,   239,   163,   132,   240,   160,   147,\n",
      "           238,   141,   147,   239,   163,   132, 30944, 31136,   238,   164,\n",
      "           160, 29871,   240,   151,   191, 30906,   240,   153,   139, 31286,\n",
      "         29871,   238,   152,   143, 31826, 29871, 30852, 31262, 30393, 29871,\n",
      "           238,   175,   191,   239,   181,   155,   238,   162,   191, 29871,\n",
      "           238,   170,   148, 31189]])\n",
      "Decoded Input IDs: 아래 문장의 비유표현을 변환하여 새로운 표현으로 다시 써 주세요..\n",
      "\n",
      "    ### Instruction:\n",
      "    육신이 흐느적흐느적하도록 피로했을 때만 정신이 은화처럼 맑소\n",
      "\n",
      "    ### Response:\n",
      "    육신이 흐느적흐느적하도록 피로했을 때만 정신이 물처럼 맑소\n",
      "Decoded Labels: 육신이 흐느적흐느적하도록 피로했을 때만 정신이 물처럼 맑소\n"
     ]
    }
   ],
   "source": [
    "# 데이터 포인트 예시\n",
    "data_point = {\n",
    "    \"context\": \"육신이 흐느적흐느적하도록 피로했을 때만 정신이 은화처럼 맑소\",\n",
    "    \"label\": \"육신이 흐느적흐느적하도록 피로했을 때만 정신이 물처럼 맑소\"\n",
    "}\n",
    "\n",
    "# 프롬프트 생성 함수\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"아래 문장의 비유표현을 변환하여 새로운 표현으로 다시 써 주세요..\n",
    "\n",
    "    ### Instruction:\n",
    "    {data_point[\"context\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"label\"]}\"\"\"\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt = generate_prompt(data_point)\n",
    "\n",
    "# 입력 문장과 목표 문장 토큰화\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "labels = tokenizer(data_point[\"label\"], return_tensors='pt').input_ids\n",
    "\n",
    "# input_ids와 labels 출력\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# 토큰을 실제 텍스트로 변환하여 확인\n",
    "print(f\"Decoded Input IDs: {tokenizer.decode(input_ids[0], skip_special_tokens=True)}\")\n",
    "print(f\"Decoded Labels: {tokenizer.decode(labels[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_label_test_bert/Similar_data_klue_large_KoELECTRA.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "Similar_raw = []\n",
    "\n",
    "for item in data[\"data\"]:\n",
    "    for para in item[\"para\"]:\n",
    "        context = para[\"context\"]\n",
    "        for labels in para[\"label\"]:\n",
    "            label = labels[\"sequence\"]\n",
    "            token_str = labels[\"token_str\"]\n",
    "            Similar_raw.append((context,label,token_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>알듯 한곳도 모르는 대목도 많은 것을 이를 악물고 시험공부하듯이 대들었으나 날이 거...</td>\n",
       "      <td>알듯 한곳도 모르는 대목도 많은 것을 이를 악물고 시험공부하듯이 대들었으나 날이 거...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>동으로 동으로 목말라 찾던 어머니인 땅이 인제 살 바치는 성찬은 이뿐이던가 저주받을...</td>\n",
       "      <td>동으로 동으로 목말라 찾던 어머니인 땅 하늘 인제 살 바치는 성찬은 하늘 뿐 하늘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>옷을 갈아입은 아내는 딴사람처럼 어여삐 보인다.</td>\n",
       "      <td>옷을 갈아입은 아내는 딴 여자 처럼 어여삐 보인다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>의복으로 말한대도 양복바지가 조선에 건너오자 이 본을 떠서 한동안은 홀태바지에 홀태...</td>\n",
       "      <td>의복으로 말한대도 양복바지가 조선에 건너오자 이 본을 떠서 한동안은 홀태바지에 홀태...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>바다 실어다 뿌리는 바람처럼 씌워 타.</td>\n",
       "      <td>바다 실어다 뿌리는 것 처럼 씌워 타.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            context  \\\n",
       "0      0  알듯 한곳도 모르는 대목도 많은 것을 이를 악물고 시험공부하듯이 대들었으나 날이 거...   \n",
       "1      1  동으로 동으로 목말라 찾던 어머니인 땅이 인제 살 바치는 성찬은 이뿐이던가 저주받을...   \n",
       "2      2                         옷을 갈아입은 아내는 딴사람처럼 어여삐 보인다.   \n",
       "3      3  의복으로 말한대도 양복바지가 조선에 건너오자 이 본을 떠서 한동안은 홀태바지에 홀태...   \n",
       "4      4                              바다 실어다 뿌리는 바람처럼 씌워 타.   \n",
       "\n",
       "                                               label  \n",
       "0  알듯 한곳도 모르는 대목도 많은 것을 이를 악물고 시험공부하듯이 대들었으나 날이 거...  \n",
       "1   동으로 동으로 목말라 찾던 어머니인 땅 하늘 인제 살 바치는 성찬은 하늘 뿐 하늘...  \n",
       "2                       옷을 갈아입은 아내는 딴 여자 처럼 어여삐 보인다.  \n",
       "3  의복으로 말한대도 양복바지가 조선에 건너오자 이 본을 떠서 한동안은 홀태바지에 홀태...  \n",
       "4                              바다 실어다 뿌리는 것 처럼 씌워 타.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw, test_data_raw = train_test_split(Similar_raw, test_size=0.1)\n",
    "\n",
    "train_data = pd.DataFrame(train_data_raw)\n",
    "test_data = pd.DataFrame(test_data_raw)\n",
    "\n",
    "train_data.rename(columns={0: 'context', 1: 'label', 2: 'token'}, inplace=True)\n",
    "test_data.rename(columns={0: 'context', 1: 'label', 2: 'token'}, inplace=True)\n",
    "train_data = train_data.drop('token', axis=1)\n",
    "train_data.reset_index(inplace=True)\n",
    "test_data.reset_index(inplace=True)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = tokenizer.bos_token\n",
    "EOS = tokenizer.eos_token\n",
    "PAD = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"아래 문장의 비유표현을 변환하여 새로운 표현으로 다시 써 주세요..\n",
    "\n",
    "    ### Instruction:\n",
    "    {data_point[\"context\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"label\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 처리 클래스\n",
    "class Similar_Dataset(Dataset):\n",
    "    def __init__(self, chats, max_len=512):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.bos= BOS\n",
    "        self.eos = EOS\n",
    "        self.pad = PAD\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):  \n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # 로드한데이터를 차례차례 DataLoader로 넘김\n",
    "        data_point = self._data.iloc[idx]\n",
    "        prompt = generate_prompt(data_point)\n",
    "        l = data_point[\"label\"]  # label을 가져온다.\n",
    "\n",
    "        prompt_toked = self.tokenizer.tokenize(self.bos + prompt + self.eos)\n",
    "        prompt_len = len(prompt_toked)\n",
    "        \n",
    "        l_toked = self.tokenizer.tokenize(self.bos + l + self.eos)\n",
    "        l_len = len(l_toked)\n",
    "\n",
    "        #최대길이 초과시 처리\n",
    "        if prompt_len> self.max_len:\n",
    "            prompt_len = self.max_len\n",
    "            prompt_toked = prompt_toked[:prompt_len]\n",
    "       \n",
    "        if l_len> self.max_len:\n",
    "            l_len = self.max_len\n",
    "            l_toked = l_toked[:l_len]\n",
    "\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(l_toked)        \n",
    "       \n",
    "        while len(labels_ids) < self.max_len: # 최대길이만큼 PADDING\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(prompt_toked)\n",
    "      \n",
    "        while len(token_ids) < self.max_len: # 최대길이만큼 PADDING\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        #질문+답변, 마스크, 답변\n",
    "        return (token_ids, labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    label = [item[1] for item in batch]\n",
    "    return torch.LongTensor(data), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4435"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = Similar_Dataset(train_data, max_len=512)\n",
    "loader = DataLoader(train_set, batch_size=4,  num_workers=0, shuffle=True, collate_fn=collate_batch,)\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Token IDs: tensor([[    1, 29871, 30860,  ..., 32000, 32000, 32000],\n",
      "        [    1, 29871, 30860,  ..., 32000, 32000, 32000],\n",
      "        [    1, 29871, 30860,  ..., 32000, 32000, 32000],\n",
      "        [    1, 29871, 30860,  ..., 32000, 32000, 32000]])\n",
      "512\n",
      "torch.Size([4, 512])\n",
      "tensor(31406)\n",
      "문\n",
      "<s> 아래 문장의 비유표현을 변환하여 새로운 표현으로 다시 써 주세요..\n",
      "\n",
      "    ### Instruction:\n",
      "    남들이 이야기하는 것처럼 저어 하늘 위의 천당에 가 계신지……, 언니는 나처럼 맞지나 않고 잘 지내는지……, 우리 언니도 나처럼 날마다 얻어맞기나 하면 어쩔까?\n",
      "\n",
      "    ### Response:\n",
      "     남들이 이야기하는 것처럼 저어 하늘 위의 천당에 가 계신지 … …, 언니는 남편 처럼 맞지 남편 않고 잘 지내는지 … …, 우리 언니도 남편 처럼 날마다 얻어맞기 남편 하면 어쩔까? <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "4\n",
      "Labels IDs: tensor([[    1,   259, 31754,  ..., 32000, 32000, 32000],\n",
      "        [    1,   259,   238,  ..., 32000, 32000, 32000],\n",
      "        [    1, 29871, 30784,  ..., 32000, 32000, 32000],\n",
      "        [    1, 29871,   239,  ..., 32000, 32000, 32000]])\n",
      "Labels IDs: <s>  남들이 이야기하는 것처럼 저어 하늘 위의 천당에 가 계신지 … …, 언니는 남편 처럼 맞지 남편 않고 잘 지내는지 … …, 우리 언니도 남편 처럼 날마다 얻어맞기 남편 하면 어쩔까? <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(loader):\n",
    "    token_ids, labels_ids = batch\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(\"Token IDs:\", token_ids)\n",
    "    print(len(token_ids[0]))\n",
    "    print(token_ids.shape)\n",
    "    print(token_ids[0][7])\n",
    "    print(tokenizer.decode(token_ids[0][7]))\n",
    "    print(tokenizer.decode(token_ids[0]))\n",
    "\n",
    "    print(len(token_ids))\n",
    "    print(\"Labels IDs:\", labels_ids)\n",
    "    print(\"Labels IDs:\", (tokenizer.decode(labels_ids[0])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              (qkv_proj): Linear(\n",
       "                in_features=3072, out_features=9216, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|          | 0/4435 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.3965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 1.2413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 1.1948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 1.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:  10%|▉         | 435/4435 [02:30<23:01,  2.89it/s, training_loss=0.541]"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "# 스케쥴러 설정\n",
    "epochs = 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(loader)*epochs)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(loader, desc=\"Epoch {:1d}\".format(epoch+1), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, labels = batch\n",
    "\n",
    "        # 모델 forward 및 loss 계산\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # 백워드 및 옵티마이저 스텝\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
    "\n",
    "    print(\"Epoch {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, total_loss / len(loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3942"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./phi_3_similar_last\")\n",
    "tokenizer.save_pretrained(\"./phi_3_similar_last\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
