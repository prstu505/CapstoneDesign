{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from kiwipiepy import Kiwi\n",
    "import json\n",
    "import re\n",
    "\n",
    "KIWI = Kiwi(num_workers=2, load_default_dict=True, model_type='knlm')\n",
    "\n",
    "SRC_PATH = ['./data/5_modern_essay', './data/5_modern_novel', './data/5_modern_poem']\n",
    "\n",
    "PATTERN = [\n",
    "    '같은', \n",
    "    '같이', \n",
    "    '처럼', \n",
    "    '와도', '과도', \n",
    "    '인 듯', '인듯', \n",
    "    '와같이', '와 같이', '과같이', '과 겉이',\n",
    "    '와같은', '와 같은', '와같이', '와 같이'\n",
    "]\n",
    "\n",
    "# 파일 저장을 위해 토큰을 사전으로 변환\n",
    "def token_to_dict(token) :\n",
    "    return {'form':token[0], 'tag':token[1], 'start':token[2], 'len':token[3]}\n",
    "\n",
    "# 동일내용 괄호 삭제\n",
    "def remove_ss(sentence) :\n",
    "    p = re.compile(r'[[|{|()}].+?[]|}|)]')\n",
    "\n",
    "    result = ''\n",
    "    m = re.search(p, sentence)\n",
    "    while m is not None : \n",
    "        in_ss = sentence[m.start()+1:m.end()-1]\n",
    "        front_ss = sentence[m.start()-len(in_ss):m.start()]\n",
    "        if in_ss == front_ss :\n",
    "            result = result + sentence[:m.start()]\n",
    "        sentence = sentence[m.end():]\n",
    "        m = re.search(p, sentence)\n",
    "    result = result + sentence\n",
    "\n",
    "    return result\n",
    "\n",
    "# 보조관념 토큰 찾기\n",
    "def find_aux_idea_tokens(pattern, sentence, tokens) : \n",
    "    result = []\n",
    "\n",
    "    si = sentence.find(pattern) # 문장에서 패턴의 인덱스\n",
    "    while si < len(sentence) :\n",
    "        if si == -1 : # 패턴을 찾지 못한 경우\n",
    "            break\n",
    "        else : # 패턴을 찾은 경우\n",
    "            ti = 0 # 토큰에서 패턴의 인덱스\n",
    "            \n",
    "            # 패턴 토큰 찾기\n",
    "            while ti < len(tokens) :\n",
    "                if tokens[ti][2] == si : break\n",
    "                else : ti = ti + 1\n",
    "\n",
    "            tag = tokens[ti-1][1] # 보조 관념의 형태소 태그\n",
    "            if tag in ['NNG', 'NNP'] : # NNG : 일반 명사 / NNP : 고유 명사\n",
    "                result.append(token_to_dict(tokens[ti-1]))\n",
    "\n",
    "        # 하나의 패턴에 대해 복수의 표현이 있을수 있으므로 탐색 위치를 조정하고 추가 탐색\n",
    "        si = sentence.find(pattern, si + len(pattern))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "output = {}\n",
    "sentence_count = 1\n",
    "for i in range(0, len(SRC_PATH), 1) :\n",
    "    # 경로와 파일이름 리스트 설정\n",
    "    src_path = SRC_PATH[i]\n",
    "    file_list = os.listdir(src_path)\n",
    "\n",
    "    for j in range(0, len(file_list), 1) :\n",
    "        file_name = file_list[j]\n",
    "        book = None\n",
    "        with open(src_path+'/'+file_name, 'r', encoding='utf-8') as file :\n",
    "            book = json.load(file)\n",
    "\n",
    "            for sentence in book['sentences'] :\n",
    "                if len(sentence) > 1000 : break # 문장 분리가 제대로 되지 않은 경우\n",
    "\n",
    "                for pattern in PATTERN : # 각 패턴마다 작업 수행\n",
    "                    if pattern in sentence :\n",
    "                        sentence = remove_ss(sentence) # 번역으로 인해 생긴 동알내용 괄호 삭제\n",
    "                        tokens = KIWI.analyze(sentence)[0][0] # 형태소 분석\n",
    "                        aux_tokens = find_aux_idea_tokens(pattern, sentence, tokens) # 패턴에 따른 보조관념 찾기\n",
    "\n",
    "                        # 문장에 패턴에 대해 유효한 보조관념을 추출한 경우\n",
    "                        if len(aux_tokens) != 0 : \n",
    "                            # json으로 쓰기위해 token을 사전형태로 변환\n",
    "                            for k in range(0, len(tokens), 1) : \n",
    "                                tokens[k] = token_to_dict(tokens[k])\n",
    "\n",
    "                            # 결과 사전에 추가\n",
    "                            output[sentence_count] = {\n",
    "                                'sentence': sentence,\n",
    "                                'aux_tokens' : aux_tokens,\n",
    "                                'tokens': tokens\n",
    "                                \n",
    "                            }\n",
    "                            sentence_count = sentence_count + 1\n",
    "\n",
    "# 파일로 저장\n",
    "with open('./data/6_modern_data.json', 'w', encoding='utf-8') as file :\n",
    "    json.dump(output, file, indent='\\t', ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93bbe001a7e75fcb6fd25285e62c9ca5fb5f81e1bceb4c82912b381d262ce5ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
